# Moscow State University - Machine Learning

1. **Основные понятия и примеры прикладных задач** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect1.pdf)
* Постановка задач обучения по прецедентам. Объекты и признаки. Типы шкал: бинарные, номинальные, порядковые, количественные.
* Типы задач: классификация, регрессия, прогнозирование, ранжирование.
* Основные понятия: модель алгоритмов, метод обучения, функция потерь и функционал качества, принцип минимизации эмпирического риска, обобщающая способность, скользящий контроль.
* Линейные модели регрессии и классификации. Метод наименьших квадратов. Полиномиальная регрессия.
* Примеры прикладных задач.
* Методика экспериментального исследования и сравнения алгоритмов на модельных и реальных данных.
* Конкурсы по анализу данных kaggle.com. Полигон алгоритмов классификации.
* CRISP-DM — межотраслевой стандарт ведения проектов интеллектуального анализа данных.

2. **Линейный классификатор и стохастический градиент** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect2.pdf)
* Линейный классификатор, модель МакКаллока-Питтса, непрерывные аппроксимации пороговой функции потерь.
* Метод стохастического градиента SG.
* Метод стохастического среднего градиента SAG.
* Эвристики: инициализация весов, порядок предъявления объектов, выбор величины градиентного шага, «выбивание» из локальных минимумов.
* Проблема мультиколлинеарности и переобучения, регуляризация или редукция весов (weight decay).
* Вероятностная постановка задачи классификации. Принцип максимума правдоподобия.
* Вероятностная интерпретация регуляризации, совместное правдоподобие данных и модели. Принцип максимума апостериорной вероятности.
* Гауссовский и лапласовский регуляризаторы.
* Логистическая регрессия. Принцип максимума правдоподобия и логарифмическая функция потерь. Метод стохастического градиента для логарифмической функции потерь. Многоклассовая логистическая регрессия. Регуляризованная логистическая регрессия. Калибровка Платта.

3. **Нейронные сети: градиентные методы оптимизации** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect3.pdf)
* Биологический нейрон, модель МакКаллока-Питтса как линейный классификатор. Функции активации.
* Проблема полноты. Задача исключающего или. Полнота двухслойных сетей в пространстве булевых функций.
* Алгоритм обратного распространения ошибок.
* Быстрые методы стохастического градиента: Поляка, Нестерова, AdaGrad, RMSProp, AdaDelta, Adam, Nadam, диагональный метод Левенберга-Марквардта.
* Проблема взрыва градиента и эвристика gradient clipping.
* Метод случайных отключений нейронов (Dropout). Интерпретации Dropout. Обратный Dropout и L2-регуляризация.
* Функции активации ReLU и PReLU. Проблема «паралича» сети.
* Эвристики для формирования начального приближения. Метод послойной настройки сети.
* Подбор структуры сети: методы постепенного усложнения сети, оптимальное прореживание нейронных сетей (optimal brain damage).

4. **Метрические методы классификации и регрессии** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect4.pdf)
* Гипотезы компактности и непрерывности.
* Обобщённый метрический классификатор.
* Метод ближайших соседей kNN и его обобщения. Подбор числа k по критерию скользящего контроля.
* Метод окна Парзена с постоянной и переменной шириной окна.
* Метод потенциальных функций и его связь с линейной моделью классификации.
* Задача отбора эталонов. Полный скользящий контроль (CCV), формула быстрого вычисления для метода 1NN. Профиль компактности.
* Отбор эталонных объектов на основе минимизации функционала полного скользящего контроля.
* Непараметрическая регрессия. Локально взвешенный метод наименьших квадратов. Ядерное сглаживание.
* Оценка Надарая-Ватсона с постоянной и переменной шириной окна. Выбор функции ядра и ширины окна сглаживания.
* Задача отсева выбросов. Робастная непараметрическая регрессия. Алгоритм LOWESS.

5. **Метод опорных векторов** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect5.pdf)
* Оптимальная разделяющая гиперплоскость. Понятие зазора между классами (margin).
* Случаи линейной разделимости и отсутствия линейной разделимости. Связь с минимизацией регуляризованного эмпирического риска. Кусочно-линейная функция потерь.
* Задача квадратичного программирования и двойственная задача. Понятие опорных векторов.
* Рекомендации по выбору константы C.
* Функция ядра (kernel functions), спрямляющее пространство, теорема Мерсера.
* Способы конструктивного построения ядер. Примеры ядер.
* SVM-регрессия.
* Регуляризации для отбора признаков: LASSO SVM, Elastic Net SVM, SFM, RFM.
* Метод релевантных векторов RVM

6. **Многомерная линейная регрессия** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect6.pdf)
* Задача регрессии, многомерная линейная регрессия.
* Метод наименьших квадратов, его вероятностный смысл и геометрический смысл.
* Сингулярное разложение.
* Проблемы мультиколлинеарности и переобучения.
* Регуляризация. Гребневая регрессия через сингулярное разложение.
* Методы отбора признаков: Лассо Тибширани, Elastic Net, сравнение с гребневой регрессией.
* Метод главных компонент и декоррелирующее преобразование Карунена-Лоэва, его связь с сингулярным разложением.
* Спектральный подход к решению задачи наименьших квадратов.
* Задачи и методы низкоранговых матричных разложений.

7. **Нелинейная регрессия** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect7.pdf)
* Метод Ньютона-Рафсона, метод Ньютона-Гаусса.
* Обобщённая аддитивная модель (GAM): метод настройки с возвращениями (backfitting) Хасти-Тибширани.
* Логистическая регрессия. Метод наименьших квадратов с итеративным пересчётом весов (IRLS). Пример прикладной задачи: кредитный скоринг. Бинаризация признаков. Скоринговые карты и оценивание вероятности дефолта. Риск кредитного портфеля банка.
* Обобщённая линейная модель (GLM). Экспоненциальное семейство распределений.
* Неквадратичные функции потерь. Метод наименьших модулей. Квантильная регрессия. Пример прикладной задачи: прогнозирование потребительского спроса.
* Робастная регрессия, функции потерь с горизонтальными асимптотами.

8. **Критерии выбора моделей и методы отбора признаков** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect8.pdf)
* Критерии качества классификации: чувствительность и специфичность, ROC-кривая и AUC, точность и полнота, AUC-PR.
* Внутренние и внешние критерии. Эмпирические и аналитические критерии.
* Скользящий контроль, разновидности эмпирических оценок скользящего контроля. Критерий непротиворечивости.
* Разновидности аналитических оценок. Регуляризация. Критерий Акаике (AIC). Байесовский информационный критерий (BIC). Оценка Вапника-Червоненкиса.
* Сложность задачи отбора признаков. Полный перебор.
* Метод добавления и удаления, шаговая регрессия.
* Поиск в глубину, метод ветвей и границ.
* Усечённый поиск в ширину, многорядный итерационный алгоритм МГУА.
* Генетический алгоритм, его сходство с МГУА.
* Случайный поиск и Случайный поиск с адаптацией (СПА).

9. **Логические методы классификации** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect9.pdf)
* Понятие логической закономерности.
* Параметрические семейства закономерностей: конъюнкции пороговых правил, синдромные правила, шары, гиперплоскости.
* Переборные алгоритмы синтеза конъюнкций: стохастический локальный поиск, стабилизация, редукция.
* Двухкритериальный отбор информативных закономерностей, парето-оптимальный фронт в (p,n)-пространстве.
* Статистический критерий информативности, точный тест Фишера. Сравнение областей эвристических и статистических закономерностей. Разнообразие критериев информативности в (p,n)-пространстве.
* Решающее дерево. Жадная нисходящая стратегия «разделяй и властвуй». Алгоритм ID3. Недостатки жадной стратегии и способы их устранения. Проблема переобучения.
* Вывод критериев ветвления. Мера нечистоты (impurity) распределения. Энтропийный критерий, критерий Джини.
* Редукция решающих деревьев: предредукция и постредукция. Алгоритм C4.5.
* Деревья регрессии. Алгоритм CART.
* Небрежные решающие деревья (oblivious decision tree).
* Решающий лес. Случайный лес (Random Forest).
* Решающий пень. Бинаризация признаков. Алгоритм разбиения области значений признака на информативные зоны.
* Решающий список. Жадный алгоритм синтеза списка. Преобразование решающего дерева в решающий список.

10. **Линейные ансамбли** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect10.pdf)
* Основные понятия: базовый алгоритм, корректирующая операция.
* Простое голосование (комитет большинства).
* Стохастические методы: бэггинг и метод случайных подпространств.
* Случайный лес (Random Forest).
* Взвешенное голосование. Преобразование простого голосования во взвешенное.
* Алгоритм AdaBoost. Экспоненциальная аппроксимация пороговой функции потерь. Процесс последовательного обучения базовых алгоритмов. Теорема о сходимости бустинга. Идентификация нетипичных объектов (выбросов).
* Теоретические обоснования. Обобщающая способность бустинга.
* Базовые алгоритмы в бустинге. Решающие пни.
* Сравнение бэггинга и бустинга.
* Алгоритм ComBoost. Обобщение на большое число классов.

11. **Продвинутые методы ансамблирования** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect11.pdf)
* Виды ансамблей. Теоретические обоснования. Анализ смещения и разброса для простого голосования.
* Градиентный бустинг. Стохастический градиентный бустинг.
* Варианты бустинга: регрессия, Алгоритм AnyBoost, GentleBoost, LogitBoost, BrownBoost, и другие.
* Алгоритм XGBoost.
* Алгоритм CatBoost. Обработка категориальных признаков.
* Стэкинг. Линейный стэкинг, взвешенный по признакам.
* Смесь алгоритмов (квазилинейная композиция), область компетентности, примеры функций компетентности.
* Выпуклые функции потерь. Методы построения смесей: последовательный и иерархический.
* Построение смеси алгоритмов с помощью EM-подобного алгоритма.

12. **Восстановление плотности распределения** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect12.pdf)
* Параметрическое оценивание плотности. Многомерное нормальное распределение, геометрическая интерпретация.
* Выборочные оценки параметров многомерного нормального распределения. Проблемы мультиколлинеарности и переобучения. Регуляризация ковариационной матрицы.
* Матричное дифференцирование. Вывод оценок параметров многомерного нормального распределения.
* Непараметрическое оценивание плотности. Ядерная оценка плотности Парзена-Розенблатта. Одномерный и многомерный случаи.
* Смесь распределений. EM-алгоритм как метод простых итераций. Обобщённый EM-алгоритм. Стохастический EM-алгоритм.
* Детали реализации EM-алгоритма. Критерий останова. Выбор начального приближения.
* Выбор числа компонентов смеси. Пошаговая стратегия. Иерархический EM-алгоритм.

13. **Байесовская теория классификации** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect13.pdf)
* Байесовская теория классификации. Оптимальный байесовский классификатор.
* Генеративные и дискриминативные модели классификации.
* Наивный байесовский классификатор. Линейный наивный байесовский классификатор в случае экспоненциального семейства распределений.
* Мультиномиальный наивный байесовский классификатор для классификации текстов.
* Метод парзеновского окна. Выбор функции ядра. Выбор ширины окна, переменная ширина окна.
* Нормальный дискриминантный анализ. Квадратичный дискриминант. Вид разделяющей поверхности. Подстановочный алгоритм, его недостатки и способы их устранения. Линейный дискриминант Фишера.
* Связь линейного дискриминанта Фишера с методом наименьших квадратов.
* Смесь многомерных нормальных распределений. Сеть радиальных базисных функций (RBF) и применение EM-алгоритма для её настройки. Сравнение RBF-сети и SVM с гауссовским ядром.

14. **Кластеризация и частичное обучение** [ссылка](https://github.com/CS-MSU/2s-ML/blob/main/lect/lect14.pdf)
* Постановка задачи кластеризации. Примеры прикладных задач. Типы кластерных структур.
* Постановка задачи Semisupervised Learning, примеры приложений.
* Оптимизационные постановки задач кластеризации и частичного обучения.
* Алгоритм k-средних и ЕМ-алгоритм для разделения гауссовской смеси.
* Алгоритм DBSCAN.
* Агломеративная кластеризация, Алгоритм Ланса-Вильямса и его частные случаи.
* Алгоритм построения дендрограммы. Определение числа кластеров.
* Свойства сжатия/растяжения и монотонности.
* Простые эвристические методы частичного обучения: self-training, co-training, co-learning.
* Трансдуктивный метод опорных векторов TSVM.
* Алгоритм Expectation-Regularization на основе многоклассовой регуляризированной логистической регрессии.